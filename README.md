# Data-Mining

## Overview

Welcome to the Data-Mining repository! This project is dedicated to data mining 'Hysterical Literature'(发疯文学).

This repository contains the work I have done as part of a larger project related to "Hysterical Literature". Below is a summary of the tasks I have completed:


### 1. Data Scraping

Scraped data from the Douyin platform related to "Hysterical Literature" including Username, User Location, Comment Text, Comment Likes, etc.

### 2. Convolutional Neural Network Classifier

Built a CNN classifier to effectively distinguish "Hysterical Literature" text from other types of text.

### 3. Association Rule Mining

Applied association rule mining techniques to uncover patterns and relationships within "Hysterical Literature" texts.

### 4. ChatGLM3 API Integration

Integrated the ChatGLM3 API to categorize "Hysterical Literature" text, including scene recognition, emotion classification, emotion scoring, and severity scoring of the hysteria.

### 5. Fine-tuning ChatGLM3

Performed full fine-tuning of the ChatGLM3 model using prompts generated by the fourth task, enabling accurate and context-specific generation of "Hysterical Literature" text.


> Please note that the project is a collaborative effort, and the part I have uploaded represents my own contribution to the overall work.

> My team members have also made significant contributions to the project, and with their consent, additional components such as the utilization of K-means clustering, hierarchical clustering, and multiple linear regression to understand the relationship between 'Hysterical Literature' data and regional variables, as well as the application of the ARIMA method for forecasting the future popularity of "Hysterical Literature," will be uploaded in the future.



## What's 'Hysterical Literature'?
- [ ] todo

## Prerequisites

Before you start, make sure you have the following:

- [chromedriver.exe](http://chromedriver.storage.googleapis.com/index.html): Download and place it in the current code path, based on your installed version of Google Chrome.

- Required Python libraries installed. You can install them using the following command:

    ```bash
    pip install wordcloud jieba jiagu keras numpy
    ```
    
- Have the following files downloaded and place them in the main directory of your project(You may find them online):

      - `cn_stopwords.txt`: The file containing Chinese stopwords for data cleaning.
  
      - `font.ttf`: The font file for the word cloud.
- [ ] todo

## Instructions

1. **Web Crawler:**
   - Open `crawler.py` and find the line: `id = "" `, replace the video ID with your interested Douyin video ID.
   - Run `crawler.py` to get comment data.
   - The crawled data will be stored in a CSV file, which will be named based on your video ID and will be used in generating word cloud.
       
2. **Generate Word Cloud:**
   - Replace the file name in `wordcloud.py` with your file name.
   - Run `wordcloud.py` to generate a word cloud pic based on the data.

3. **Analyze Emotion**
   - Replace the text with your text.
   - Run `emotion_analysis.py` to get the sentiment of the text.
     
4. **CNN Classifier**
   - Replace the csv with your data in `opinion_analysis/demo.py`
   - Run `opinion_analysis/demo.py`
   - Current hyperparameter:
       - Learning rate：0.001
       - Epochs：15
       - Batch size：128
   - Example result:
  
     
     <img src="https://github.com/oan-J/Data-Mining/blob/main/img/loss.png" alt="loss pic" width="300">
  
     
     <img src="https://github.com/oan-J/Data-Mining/blob/main/img/acc.png" alt="acc pic" width="300">

- [ ] todo
